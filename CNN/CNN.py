# -*- coding: utf-8 -*-
"""7_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y5bSSfQReHB78I5abiOROtRuItsxL-q9

# CNN

En el siguiente proyecto entrenaremos un algoritmo que buscará determinar, dada una foto de una mano, qué número (del 0 al 5) está haciendo en lenguaje internacional de señas. Para esto utilizaremos una porción del dataset SIGNS creado por [deeplearning.ai](http://deeplearning.ai) para una de sus especializaciones.
"""

#Tomo los datasets de un archivo en mi Drive de Google:

#Permito que este cuadernillo entre a mis archivos en Google Drive:
from google.colab import drive

#Por las dudas que ya se haya ejecutado este bloque borro todos los cambios:
drive.flush_and_unmount()

#el módulo drive nos permite acceder a nuestros archivos de Google Drive mediante la función mount:
drive.mount('/content/drive')

"""En la siguiente celda hemos cargado una función que carga los datos por usted:"""

import h5py
import numpy as np
def load_dataset(path=''):
    train_dataset = h5py.File(path+'train_signs.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File(path+'test_signs.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes

X_train_full, y_train_full, X_test, y_test, classes = load_dataset('/content/drive/MyDrive/ML/')
y_train_full = y_train_full.reshape(-1,1)
y_test = y_test.reshape(-1,1)
print("Lectura completa")

"""Si la lectura fue efectuada correctamente, podremos graficar 10 elementos del dataset utilizando el siguiente iterador:"""

import matplotlib.pyplot as plt

n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.title(str(y_train_full[i,0]))
    plt.imshow(X_train_full[i])
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

"""## Ejercicio 1: Preparación de datos

Intente realizar las siguientes operaciones:

*  ¿Qué shape tienen los datos? ¿Cuántas muestras hay? ¿Qué rango de valores tiene cada pixel?
*  Normalice los datos apropiadamente para entrenar usando una red neuronal.
*  Separe del conjunto de entrenamiento un 20% de los datos para utilizar como datos de validación.
"""

from sklearn.model_selection import train_test_split

#Respuesta:
print(classes)
print(X_train_full.shape)
print(X_test.shape)

print(np.min(X_train_full))
print(np.max(X_train_full))

X_train_full = X_train_full/255
X_test = X_test/255

X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)

"""## 2. Construcción de la Red

Para analizarlo, construiremos un modelo en Keras con una estructura inspirada en [AlexNet (2012)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). Esta consistirá en tres bloques convolucionales, seguida de dos layers densos para hacer la clasificación.

Cada bloque convolucional estará constituído por:
* Capa convolucional 2D con activación `ReLu`, tamaño de ventana (3,3), stride de (1,1) y con un padding `SAME` para mantener las dimensiones del output iguales a las del input.
* Capa de MaxPooling2D con un tamaño de ventana de (2,2), stride de (2,2) y con padding `VALID`, para reducir las dimensiones del output a la mitad.

La red consistirá de tres bloques convolucionales (cada uno conformado por dos capas, como se describió más arriba) seguidos de capas densas y capas de regularización Dropout. El esquema es el siguiente:
1. Bloque Convolucional de 16 filtros
2. Bloque Convolucional de 32 filtros
3. Bloque Convolucional de 64 filtros
4. Dropout del 40%
5. Una capa de 524 neuronas, con activación ReLu
6. Dropout del 40%
7. Una capa de 6 neuronas con activación Softmax, para hacer la clasificación.

Tenga en cuenta que entre los bloques convolucionales y las capas densas (entre items 4 y 5), se deberá aplicar una capa Flatten que pase el output 3D de las capas convolucionales a un vector 1D apto para las capas densas.

A continuación, construiremos dicho modelo utilizando la API secuencial o funcional de Keras, segun su preferencia. ¿Cuántos parámetros tiene su modelo?
"""

import tensorflow as tf
from tensorflow import keras

keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

input_ = keras.layers.Input(shape=X_train.shape[1:]) #En este caso es necesario definir el Layer de Inputs

conv1 = keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same', input_shape=X_train.shape[1:], activation='relu')(input_)
pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid')(conv1)

conv2 = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(pool1)
pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid')(conv2)

conv3 = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(pool2)
pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid')(conv3)

#dropout1 = keras.layers.Dropout(0.4)(pool3)
flatten = keras.layers.Flatten()(pool3)

hidden1 = keras.layers.Dense(524, activation="relu")(flatten)

#dropout2 = keras.layers.Dropout(0.4)(hidden1)
output = keras.layers.Dense(6, activation="softmax")(hidden1)


model = keras.models.Model(inputs=[input_], outputs=[output])

"""Ejercicio 2. Compilación
Una vez definida la arquitectura de la red, intente compilarla. ¿Cuánto tiempo demoró su compilación? Puede imprimir por pantalla sus características?
"""

model.compile(loss="sparse_categorical_crossentropy",
              optimizer="adam",
              metrics=["accuracy"])

model.summary()

keras.utils.plot_model(model, "signs_cnn.png", show_shapes=True)

"""## Ejercicio 3: Entrenamiento

Entrene la red generada utilizando un parámetro de epochs = 50, con un batch_size de 32
"""

history = model.fit(X_train, y_train, epochs=20, batch_size=32,
                    validation_data=(X_valid, y_valid))

import pandas as pd

pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

"""## Ejercicio 4: Evaluación

Evalúe el accuracy de su modelo y la matriz de confusión del mismo.
Por último, grafique algunas predicciones junto con la imagen que corresponde a dicha predicción.
"""

from sklearn import metrics

y_proba = model.predict(X_test)
y_pred = np.argmax(y_proba, axis=-1)

print("Accuracy: ", metrics.accuracy_score(y_test, y_pred))

metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize=None)

plt.show()

print("Los primeros 10:")

n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.title(str(y_pred[i]))
    plt.imshow(X_test[i])
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

#Grafico en los que se equivocó
y_pred_err = y_pred.reshape(120) != y_test.reshape(120)

print("Los que se equivocó:")

n = y_pred_err.shape[0]
plt.figure(figsize=(20, 4))
j = 0
for i in range(n):
    if y_pred_err[i] == True:
        ax = plt.subplot(2, np.count_nonzero(y_pred_err) , j + 1)
        j = j + 1
        plt.title(str(y_pred[i]))
        plt.imshow(X_test[i])
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
plt.show()

"""## Reentrene el modelos con los valores de learning_rate de 0.01, 0.001, 0.0001, 0.00001.¿Qué resultados puede concluir? Se recomienda a medida que disminuye el learning rate aumentar la cantidad de épocas.

"""

optimizer = keras.optimizers.Adam(learning_rate=0.00001)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer= optimizer,
              metrics=["accuracy"])

model.summary()

history = model.fit(X_train, y_train, epochs=20, batch_size=16,
                    validation_data=(X_valid, y_valid))

import pandas as pd

pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

from sklearn import metrics

y_proba = model.predict(X_test)
y_pred = np.argmax(y_proba, axis=-1)

print("Accuracy: ", metrics.accuracy_score(y_test, y_pred))

metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize=None)

plt.show()

"""Al modificar disminuyendo los valores de learning_rate, la probabilidad de que mejore el modelo es mayor y disminuye el sobreajuste, ya que tarda más en ajustar, pero genera mejores resultados.

## Analice la matriz de confusión y el accuracy obtenidos. ¿Qué modificaciones haría en el modelo para mejorar los resultados?

Las modificaciones que se analizan según disminuye el error de entrenamiento y aumentar la exactitud, en función de la matriz de confusión y el accuracy obtenidos, seria probar cambiar el optimizador, modificar el valor “bach” dependiendo la cantidad del set de datos total y proponer un “early stopping”, que hace que el entrenamiento que frene cuando sobre ajuste.
"""